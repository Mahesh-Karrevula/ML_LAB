{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41858f39",
      "metadata": {
        "id": "41858f39"
      },
      "source": [
        "\n",
        "# üöó Cars Dataset ‚Äî Linear Regression (Step-by-Step, Beginner Friendly)\n",
        "*Generated on 2025-09-12 03:29:55*\n",
        "\n",
        "This notebook teaches **Linear Regression** with a Cars dataset in **small, easy steps**.  \n",
        "Each step ends with a short **üìù TODO** so you can practice.\n",
        "\n",
        "What you'll do:\n",
        "1) Download + load the dataset (via **gdown**)  \n",
        "2) Inspect + clean data (simple)  \n",
        "3) Univariate plots (histograms)  \n",
        "4) Single-variable Linear Regression (fit + plot)  \n",
        "5) Multi-variable Linear Regression (fit + metrics)  \n",
        "6) Diagnostic plots (residuals)  \n",
        "7) Polynomial features + **GridSearchCV** to find the **best degree**  \n",
        "8) Brief coefficient interpretation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdffcadd",
      "metadata": {
        "id": "fdffcadd"
      },
      "source": [
        "## 0) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cb8e4946",
      "metadata": {
        "id": "cb8e4946"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Install & import (safe to re-run)\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install(pkg):\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg], check=False)\n",
        "\n",
        "pip_install(\"gdown>=5.1\")\n",
        "pip_install(\"pandas>=1.5\")\n",
        "pip_install(\"numpy>=1.23\")\n",
        "pip_install(\"matplotlib>=3.7\")\n",
        "pip_install(\"scikit-learn>=1.3\")\n",
        "\n",
        "import gdown, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "307ee8a7",
      "metadata": {
        "id": "307ee8a7"
      },
      "source": [
        "## 1) Download & Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5c3cd0e",
      "metadata": {
        "id": "c5c3cd0e"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Download CSV via gdown\n",
        "GDOWN_ID = \"1bwRmKkPwmLKiqOgQ_LnKH0Vsc3mJKmVR\"  # provided ID\n",
        "OUTPUT_CSV = \"cars.csv\"\n",
        "\n",
        "if not os.path.exists(OUTPUT_CSV):\n",
        "    url = f\"https://drive.google.com/uc?id={GDOWN_ID}\"\n",
        "    gdown.download(url, OUTPUT_CSV, quiet=False)\n",
        "else:\n",
        "    print(\"Found existing file:\", OUTPUT_CSV)\n",
        "\n",
        "df = pd.read_csv(OUTPUT_CSV)\n",
        "print(\"‚úÖ Loaded:\", df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48462d17",
      "metadata": {
        "id": "48462d17"
      },
      "source": [
        "\n",
        "### üìù TODO\n",
        "- Skim the first few rows above. What looks like a good **target** (y) for prediction (e.g., `mpg`, `price`)?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET = \"mpg\"\n",
        "\n",
        "if TARGET not in df.columns:\n",
        "    numeric_cols_all = df.select_dtypes(include=np.number).columns.tolist()\n",
        "    if numeric_cols_all:\n",
        "        TARGET = numeric_cols_all[0]\n",
        "        print(\"Auto-selected TARGET =\", TARGET)\n",
        "\n",
        "numeric_cols = [c for c in df.select_dtypes(include=np.number).columns if c != TARGET]\n",
        "\n",
        "print(\"Target:\", TARGET)\n",
        "print(\"Numeric features (first 6 shown):\", numeric_cols[:6])\n"
      ],
      "metadata": {
        "id": "roAtCLfGl_7u"
      },
      "id": "roAtCLfGl_7u",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "505d4e37",
      "metadata": {
        "id": "505d4e37"
      },
      "source": [
        "## 2) Quick Inspect & Simple Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e0e3f9",
      "metadata": {
        "id": "71e0e3f9"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Columns:\", list(df.columns))\n",
        "print(\"\\nData types:\\n\", df.dtypes)\n",
        "print(\"\\nMissing values per column:\\n\", df.isna().sum())\n",
        "\n",
        "# Simple cleaning: drop duplicates\n",
        "before = df.shape[0]\n",
        "df = df.drop_duplicates().reset_index(drop=True)\n",
        "print(\"\\nDropped duplicates:\", before - df.shape[0])\n",
        "\n",
        "# (Optional) strip/underscore column names\n",
        "df.columns = [c.strip().replace(\" \", \"_\") for c in df.columns]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a18ecb52",
      "metadata": {
        "id": "a18ecb52"
      },
      "source": [
        "\n",
        "### üìù TODO\n",
        "- If you see obvious bad rows (e.g., impossible negative values), write one **extra line** to filter them out.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Before cleaning: {df.shape[0]} rows\")\n",
        "positive_cols = ['mpg', 'horsepower', 'weight', 'displacement']\n",
        "for col in positive_cols:\n",
        "    if col in df.columns:\n",
        "        bad_rows = df[col] <= 0\n",
        "        if bad_rows.any():\n",
        "            print(f\"Removing {bad_rows.sum()} rows with negative {col}\")\n",
        "            df = df[~bad_rows]\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_cols:\n",
        "    if col in df.columns:\n",
        "        mean_val = df[col].mean()\n",
        "        std_val = df[col].std()\n",
        "        outliers = (df[col] < mean_val - 3 * std_val) | (df[col] > mean_val + 3 * std_val)\n",
        "        if outliers.any():\n",
        "            print(f\"Removing {outliers.sum()} extreme outliers from {col}\")\n",
        "            df = df[~outliers]\n",
        "print(f\"After cleaning: {df.shape[0]} rows\")\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "n6AwR2ALqXrb"
      },
      "id": "n6AwR2ALqXrb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4f93a674",
      "metadata": {
        "id": "4f93a674"
      },
      "source": [
        "## 3) Choose Target and Features (Keep it Simple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80003962",
      "metadata": {
        "id": "80003962"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üëâ Set your target (change this if needed)\n",
        "TARGET = \"mpg\"  # <-- change if your dataset uses a different target\n",
        "\n",
        "# If the target isn't present, try to auto-pick a numeric column\n",
        "if TARGET not in df.columns:\n",
        "    numeric_cols_all = df.select_dtypes(include=np.number).columns.tolist()\n",
        "    if numeric_cols_all:\n",
        "        TARGET = numeric_cols_all[0]\n",
        "        print(\"Auto-selected TARGET =\", TARGET)\n",
        "\n",
        "# We'll work **only with numeric predictors** for simplicity.\n",
        "numeric_cols = [c for c in df.select_dtypes(include=np.number).columns if c != TARGET]\n",
        "\n",
        "# Quick sanity\n",
        "print(\"Target:\", TARGET)\n",
        "print(\"Numeric features (first 6 shown):\", numeric_cols[:6])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21dfa346",
      "metadata": {
        "id": "21dfa346"
      },
      "source": [
        "\n",
        "### üìù TODO\n",
        "- If you prefer different features, create a **manual list**, e.g.  \n",
        "  `numeric_cols = ['horsepower','weight','displacement','acceleration']` (only if those exist).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if TARGET in df.columns:\n",
        "    correlations = df[numeric_cols + [TARGET]].corr()[TARGET].abs().sort_values(ascending=False)\n",
        "    print(f\"Feature correlations with {TARGET}:\")\n",
        "    print(correlations)\n",
        "    top_features = correlations.drop(TARGET).head(3).index.tolist()\n",
        "    print(f\"\\nTop 3 most correlated features: {top_features}\")\n",
        "    numeric_cols = top_features\n",
        "    print(f\"Using features: {numeric_cols}\")\n",
        "else:\n",
        "    print(\"Target not found, using all numeric features\")"
      ],
      "metadata": {
        "id": "w67_52orq3_Z"
      },
      "id": "w67_52orq3_Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "655c216e",
      "metadata": {
        "id": "655c216e"
      },
      "source": [
        "## 4) Univariate Plots (Matplotlib)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa0248ad",
      "metadata": {
        "id": "aa0248ad"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot target histogram\n",
        "if TARGET in df.columns and pd.api.types.is_numeric_dtype(df[TARGET]):\n",
        "    plt.figure()\n",
        "    df[TARGET].plot(kind='hist', bins=30, title=f\"Histogram of {TARGET}\")\n",
        "    plt.xlabel(TARGET); plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot up to 3 numeric features' histograms\n",
        "for col in numeric_cols[:3]:\n",
        "    plt.figure()\n",
        "    df[col].plot(kind='hist', bins=30, title=f\"Histogram of {col}\")\n",
        "    plt.xlabel(col); plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32de8ec9",
      "metadata": {
        "id": "32de8ec9"
      },
      "source": [
        "\n",
        "### üìù TODO\n",
        "- Looking at the histograms, note any skewed variables or outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Distribution Analysis:\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "for col in [TARGET] + numeric_cols:\n",
        "    if col in df.columns:\n",
        "        data = df[col].dropna()\n",
        "        skewness = data.skew()\n",
        "        print(f\"{col}:\")\n",
        "        print(f\"  Skewness: {skewness:.2f}\")\n",
        "        if abs(skewness) > 1:\n",
        "            print(f\"  ‚ö†Ô∏è  Heavily skewed\")\n",
        "        elif abs(skewness) > 0.5:\n",
        "            print(f\"  ‚ö†Ô∏è  Moderately skewed\")\n",
        "        else:\n",
        "            print(f\"  ‚úÖ Approximately normal\")\n",
        "\n",
        "print(\"\\nNote: Heavily skewed variables may need transformation\")"
      ],
      "metadata": {
        "id": "HSUG4rORrHBR"
      },
      "id": "HSUG4rORrHBR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4615337b",
      "metadata": {
        "id": "4615337b"
      },
      "source": [
        "## 5) Single-Variable Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "377a2d1b",
      "metadata": {
        "id": "377a2d1b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Pick one feature (auto-pick the first numeric feature)\n",
        "if len(numeric_cols) == 0:\n",
        "    raise ValueError(\"No numeric predictors found. Please adjust `numeric_cols`.\")\n",
        "\n",
        "FEATURE_X = numeric_cols[0]  # change to try others\n",
        "\n",
        "# Drop rows with missing target/feature\n",
        "data_1v = df[[FEATURE_X, TARGET]].dropna().copy()\n",
        "\n",
        "X = data_1v[[FEATURE_X]].values\n",
        "y = data_1v[TARGET].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit linear regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "print(\"Feature:\", FEATURE_X)\n",
        "print(\"Coefficient (slope):\", lr.coef_[0])\n",
        "print(\"Intercept:\", lr.intercept_)\n",
        "print(\"R^2 (test):\", r2_score(y_test, y_pred))\n",
        "\n",
        "# Plot scatter + regression line (on test set)\n",
        "plt.figure()\n",
        "plt.scatter(X_test, y_test, label=\"Actual\")\n",
        "# create a line\n",
        "x_line = np.linspace(X.min(), X.max(), 200).reshape(-1,1)\n",
        "y_line = lr.predict(x_line)\n",
        "plt.plot(x_line, y_line, label=\"Fit\")\n",
        "plt.xlabel(FEATURE_X); plt.ylabel(TARGET); plt.title(\"Single-Variable Linear Regression\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c813aaf",
      "metadata": {
        "id": "7c813aaf"
      },
      "source": [
        "\n",
        "### üìù TODO\n",
        "- Change `FEATURE_X` to a different column and re-run.\n",
        "- Does the line slope match your intuition about the relationship?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Single Feature Comparison:\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "best_r2 = 0\n",
        "best_feature = None\n",
        "\n",
        "for feature in numeric_cols:\n",
        "    if feature == TARGET:\n",
        "        continue\n",
        "    data_test = df[[feature, TARGET]].dropna().copy()\n",
        "    X_test_feat = data_test[[feature]].values\n",
        "    y_test_feat = data_test[TARGET].values\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X_test_feat, y_test_feat, test_size=0.2, random_state=42)\n",
        "    lr_test = LinearRegression()\n",
        "    lr_test.fit(X_tr, y_tr)\n",
        "    y_pred_test = lr_test.predict(X_te)\n",
        "    r2_test = r2_score(y_te, y_pred_test)\n",
        "    print(f\"{feature:15s}: R¬≤ = {r2_test:.3f}\")\n",
        "    if r2_test > best_r2:\n",
        "        best_r2 = r2_test\n",
        "        best_feature = feature\n",
        "\n",
        "print(f\"\\nüèÜ Best feature: {best_feature} (R¬≤ = {best_r2:.3f})\")\n",
        "FEATURE_X = best_feature"
      ],
      "metadata": {
        "id": "RJjjDZZyru05"
      },
      "id": "RJjjDZZyru05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dd8880fb",
      "metadata": {
        "id": "dd8880fb"
      },
      "source": [
        "## 6) Multi-Variable Linear Regression (Simple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4b5f2c",
      "metadata": {
        "id": "fc4b5f2c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Keep only numeric predictors + target; drop NA\n",
        "data_mv = df[numeric_cols + [TARGET]].dropna().copy()\n",
        "\n",
        "X = data_mv[numeric_cols].values\n",
        "y = data_mv[TARGET].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "lin = LinearRegression()\n",
        "lin.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lin.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"MAE:  {mae:.3f}\")\n",
        "print(f\"MSE:  {mse:.3f}\")\n",
        "print(f\"RMSE: {rmse:.3f}\")\n",
        "print(f\"R^2:  {r2:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2800867",
      "metadata": {
        "id": "c2800867"
      },
      "source": [
        "\n",
        "### üìù TODO\n",
        "- Remove one weak feature from `numeric_cols` and see how metrics change.\n",
        "- Add a different feature and compare.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Feature Combination Testing:\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Original model (all features)\n",
        "data_orig = df[numeric_cols + [TARGET]].dropna().copy()\n",
        "X_orig = data_orig[numeric_cols].values\n",
        "y_orig = data_orig[TARGET].values\n",
        "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42)\n",
        "lin_orig = LinearRegression()\n",
        "lin_orig.fit(X_train_orig, y_train_orig)\n",
        "y_pred_orig = lin_orig.predict(X_test_orig)\n",
        "r2_orig = r2_score(y_test_orig, y_pred_orig)\n",
        "print(f\"All features: R¬≤ = {r2_orig:.3f}\")\n",
        "\n",
        "# Test with only top 2 features\n",
        "correlations = df[numeric_cols + [TARGET]].corr()[TARGET].abs().sort_values(ascending=False)\n",
        "top_2_features = correlations.drop(TARGET).head(2).index.tolist()\n",
        "data_top2 = df[top_2_features + [TARGET]].dropna().copy()\n",
        "X_top2 = data_top2[top_2_features].values\n",
        "y_top2 = data_top2[TARGET].values\n",
        "X_train_top2, X_test_top2, y_train_top2, y_test_top2 = train_test_split(X_top2, y_top2, test_size=0.2, random_state=42)\n",
        "lin_top2 = LinearRegression()\n",
        "lin_top2.fit(X_train_top2, y_train_top2)\n",
        "y_pred_top2 = lin_top2.predict(X_test_top2)\n",
        "r2_top2 = r2_score(y_test_top2, y_pred_top2)\n",
        "print(f\"Top 2 features: R¬≤ = {r2_top2:.3f}\")\n",
        "\n",
        "# Summary\n",
        "print(f\"\\nBest model: {'Top 2 features' if r2_top2 > r2_orig else 'All features'}\")\n",
        "if r2_top2 > r2_orig:\n",
        "    numeric_cols = top_2_features"
      ],
      "metadata": {
        "id": "GeKhtEYJr-K3"
      },
      "id": "GeKhtEYJr-K3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "944fe2a1",
      "metadata": {
        "id": "944fe2a1"
      },
      "source": [
        "## 7) Diagnostic Plots (Residuals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bfe20c7",
      "metadata": {
        "id": "0bfe20c7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Reuse y_test and y_pred from multi-variable model\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# Predicted vs Actual\n",
        "plt.figure()\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel(\"Actual\"); plt.ylabel(\"Predicted\")\n",
        "plt.title(\"Predicted vs Actual (Test)\")\n",
        "plt.show()\n",
        "\n",
        "# Residuals vs Predicted\n",
        "plt.figure()\n",
        "plt.scatter(y_pred, residuals)\n",
        "plt.axhline(0, linestyle='--')\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs Predicted\")\n",
        "plt.show()\n",
        "\n",
        "# Histogram of residuals\n",
        "plt.figure()\n",
        "pd.Series(residuals).plot(kind='hist', bins=30, title=\"Histogram of Residuals\")\n",
        "plt.xlabel(\"Residual\"); plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d515f1e",
      "metadata": {
        "id": "2d515f1e"
      },
      "source": [
        "\n",
        "### üìù TODO\n",
        "- Do residuals look roughly centered around 0 and evenly spread?\n",
        "- If not, which assumption might be violated?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Residual Analysis:\")\n",
        "print(\"=\" * 25)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# Basic checks\n",
        "print(f\"Mean of residuals: {residuals.mean():.4f} (should be ‚âà 0)\")\n",
        "print(f\"Residuals centered: {'‚úÖ YES' if abs(residuals.mean()) < 0.1 else '‚ùå NO'}\")\n",
        "\n",
        "# Visual checks\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# Residuals vs Predicted\n",
        "axes[0].scatter(y_pred, residuals, alpha=0.6)\n",
        "axes[0].axhline(0, color='red', linestyle='--')\n",
        "axes[0].set_xlabel('Predicted Values')\n",
        "axes[0].set_ylabel('Residuals')\n",
        "axes[0].set_title('Residuals vs Predicted')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Histogram of residuals\n",
        "axes[1].hist(residuals, bins=15, alpha=0.7, edgecolor='black')\n",
        "axes[1].set_xlabel('Residuals')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Distribution of Residuals')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Simple normality check\n",
        "from scipy import stats\n",
        "shapiro_stat, shapiro_p = stats.shapiro(residuals)\n",
        "print(f\"Normality test p-value: {shapiro_p:.4f}\")\n",
        "print(f\"Residuals normal: {'‚úÖ YES' if shapiro_p > 0.05 else '‚ùå NO'}\")\n",
        "\n",
        "print(\"\\nNote: If assumptions violated, consider transformations\")"
      ],
      "metadata": {
        "id": "yzToTYrjsIoH"
      },
      "id": "yzToTYrjsIoH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "05fbe0a3",
      "metadata": {
        "id": "05fbe0a3"
      },
      "source": [
        "## 8) Polynomial Features + Grid Search for Best Degree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9418dd4",
      "metadata": {
        "id": "a9418dd4"
      },
      "source": [
        "\n",
        "Even though it's called \"polynomial regression\", it's still **linear regression** applied to **polynomially-expanded features**.\n",
        "We'll try degrees **1 to 5** on a **single predictor** and pick the degree with the best cross-validated **R¬≤**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d68dfb0",
      "metadata": {
        "id": "5d68dfb0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Choose a single predictor again (same FEATURE_X as before by default)\n",
        "FEATURE_X = FEATURE_X  # keep same, or set to something else like 'horsepower'\n",
        "\n",
        "poly_data = df[[FEATURE_X, TARGET]].dropna().copy()\n",
        "X = poly_data[[FEATURE_X]].values\n",
        "y = poly_data[TARGET].values\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"poly\", PolynomialFeatures(include_bias=False)),\n",
        "    (\"lr\", LinearRegression())\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    \"poly__degree\": [1, 2, 3, 4, 5],\n",
        "    \"lr__fit_intercept\": [True, False]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(pipe, param_grid, cv=5, scoring=\"r2\", n_jobs=-1)\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Best CV R^2:\", grid.best_score_)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# Plot best curve\n",
        "x_line = np.linspace(X.min(), X.max(), 200).reshape(-1,1)\n",
        "y_line = best_model.predict(x_line)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X, y, s=12)\n",
        "plt.plot(x_line, y_line)\n",
        "plt.xlabel(FEATURE_X); plt.ylabel(TARGET)\n",
        "plt.title(\"Best Polynomial Fit (via Grid Search)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a21b23f2",
      "metadata": {
        "id": "a21b23f2"
      },
      "source": [
        "\n",
        "### üìù TODO\n",
        "- Change the degree range (e.g., 1‚Äì8) and re-run. Does performance keep improving?\n",
        "- Try a different `FEATURE_X`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Polynomial Degree Testing:\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Use the best single feature\n",
        "poly_data = df[[FEATURE_X, TARGET]].dropna().copy()\n",
        "X_poly = poly_data[[FEATURE_X]].values\n",
        "y_poly = poly_data[TARGET].values\n",
        "\n",
        "# Test degrees 1, 2, 3\n",
        "degrees = [1, 2, 3]\n",
        "results = []\n",
        "\n",
        "for degree in degrees:\n",
        "    # Create polynomial features\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_poly_transformed = poly.fit_transform(X_poly)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(\n",
        "        X_poly_transformed, y_poly, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Fit model\n",
        "    lr_poly = LinearRegression()\n",
        "    lr_poly.fit(X_train_poly, y_train_poly)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_poly = lr_poly.predict(X_test_poly)\n",
        "    r2_poly = r2_score(y_test_poly, y_pred_poly)\n",
        "\n",
        "    results.append((degree, r2_poly))\n",
        "    print(f\"Degree {degree}: R¬≤ = {r2_poly:.3f}\")\n",
        "\n",
        "# Find best degree\n",
        "best_degree, best_r2 = max(results, key=lambda x: x[1])\n",
        "print(f\"\\nüèÜ Best degree: {best_degree} (R¬≤ = {best_r2:.3f})\")\n",
        "\n",
        "# Plot the best polynomial fit\n",
        "poly_best = PolynomialFeatures(degree=best_degree, include_bias=False)\n",
        "X_poly_best = poly_best.fit_transform(X_poly)\n",
        "lr_best = LinearRegression()\n",
        "lr_best.fit(X_poly_best, y_poly)\n",
        "\n",
        "# Create smooth line for plotting\n",
        "x_line = np.linspace(X_poly.min(), X_poly.max(), 200).reshape(-1, 1)\n",
        "x_line_poly = poly_best.fit_transform(x_line)\n",
        "y_line = lr_best.predict(x_line_poly)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X_poly, y_poly, alpha=0.6, label='Data')\n",
        "plt.plot(x_line, y_line, 'r-', linewidth=2, label=f'Degree {best_degree} fit')\n",
        "plt.xlabel(FEATURE_X)\n",
        "plt.ylabel(TARGET)\n",
        "plt.title(f'Best Polynomial Fit (Degree {best_degree})')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xqaNobTmscnf"
      },
      "id": "xqaNobTmscnf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "82a5cd04",
      "metadata": {
        "id": "82a5cd04"
      },
      "source": [
        "## 9) Interpreting Coefficients (Multi-Variable Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d60c8aa",
      "metadata": {
        "id": "9d60c8aa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Show top coefficients by magnitude (multi-variable linear model)\n",
        "coef = lin.coef_\n",
        "coef_df = pd.DataFrame({\"feature\": numeric_cols, \"coefficient\": coef})\n",
        "coef_df[\"abs_coef\"] = coef_df[\"coefficient\"].abs()\n",
        "coef_df.sort_values(\"abs_coef\", ascending=False, inplace=True)\n",
        "coef_df.drop(columns=[\"abs_coef\"], inplace=True)\n",
        "coef_df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5069016a",
      "metadata": {
        "id": "5069016a"
      },
      "source": [
        "\n",
        "### üìù TODO\n",
        "- Which features have the largest (absolute) coefficients?\n",
        "- Do the signs (+/-) match your intuition?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Coefficient Analysis:\")\n",
        "print(\"=\" * 25)\n",
        "\n",
        "# Get coefficients from the multi-variable model\n",
        "coef = lin.coef_\n",
        "feature_names = numeric_cols\n",
        "\n",
        "# Create coefficient dataframe\n",
        "coef_df = pd.DataFrame({\n",
        "    \"feature\": feature_names,\n",
        "    \"coefficient\": coef,\n",
        "    \"abs_coefficient\": np.abs(coef)\n",
        "})\n",
        "\n",
        "# Sort by absolute coefficient value\n",
        "coef_df_sorted = coef_df.sort_values(\"abs_coefficient\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"Feature coefficients:\")\n",
        "for idx, row in coef_df_sorted.iterrows():\n",
        "    feature = row['feature']\n",
        "    coef_val = row['coefficient']\n",
        "    direction = \"positive\" if coef_val > 0 else \"negative\"\n",
        "    print(f\"{feature:15s}: {coef_val:8.4f} ({direction})\")\n",
        "    if coef_val > 0:\n",
        "        print(f\"                 ‚Üí Higher {feature} ‚Üí Higher {TARGET}\")\n",
        "    else:\n",
        "        print(f\"                 ‚Üí Higher {feature} ‚Üí Lower {TARGET}\")\n",
        "\n",
        "# Simple visualization\n",
        "plt.figure(figsize=(8, 5))\n",
        "colors = ['red' if c < 0 else 'blue' for c in coef_df_sorted['coefficient']]\n",
        "bars = plt.barh(coef_df_sorted['feature'], coef_df_sorted['coefficient'], color=colors, alpha=0.7)\n",
        "plt.axvline(0, color='black', linestyle='-', linewidth=0.8)\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Feature Coefficients\\n(Red = Negative, Blue = Positive)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nMost important feature: {coef_df_sorted.iloc[0]['feature']}\")\n",
        "print(f\"Largest coefficient: {coef_df_sorted.iloc[0]['coefficient']:.4f}\")"
      ],
      "metadata": {
        "id": "azsm2caqsoTZ"
      },
      "id": "azsm2caqsoTZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6a4273bb",
      "metadata": {
        "id": "6a4273bb"
      },
      "source": [
        "\n",
        "## 10) Linear Regression ‚Äî Quick Assumptions Checklist\n",
        "- **Linearity**: Relationship between predictors and target is roughly linear.\n",
        "- **Independence**: Errors are independent.\n",
        "- **Homoscedasticity**: Residuals have constant variance.\n",
        "- **Normality (for inference)**: Residuals are roughly normal.\n",
        "- **No perfect multicollinearity**: Avoid duplicate/linearly dependent features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "414d56ff",
      "metadata": {
        "id": "414d56ff"
      },
      "source": [
        "\n",
        "## 11) Assumption Checks ‚Äî Code You Can Run\n",
        "We'll check the classic linear regression assumptions using simple, readable code:\n",
        "- **Linearity & Homoscedasticity:** residuals vs predicted plot\n",
        "- **Normality of residuals:** Q‚ÄìQ plot and Shapiro‚ÄìWilk test\n",
        "- **Independence of errors:** Durbin‚ÄìWatson statistic\n",
        "- **Multicollinearity:** Variance Inflation Factor (VIF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d819a91",
      "metadata": {
        "id": "6d819a91"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Install test libs (statsmodels) if needed\n",
        "import sys, subprocess\n",
        "def pip_install(pkg):\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg], check=False)\n",
        "\n",
        "try:\n",
        "    import statsmodels\n",
        "except:\n",
        "    pip_install(\"statsmodels>=0.14\")\n",
        "    import statsmodels\n",
        "\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# NOTE: This cell assumes you've already run the multi-variable model section\n",
        "# so that y_test, y_pred, X_train, X_test, numeric_cols, TARGET are defined.\n",
        "# If not, re-run sections 6 and 7.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f55aae1",
      "metadata": {
        "id": "0f55aae1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 11.1 Linearity & Homoscedasticity (visual)\n",
        "# Residuals vs Predicted should look like a random cloud around 0 (no pattern / fanning)\n",
        "\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(y_pred, residuals, s=14)\n",
        "plt.axhline(0, linestyle=\"--\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs Predicted\")\n",
        "plt.show()\n",
        "\n",
        "print(\"üßê Look for: no obvious curve/pattern; spread roughly constant across x-axis.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59db51f7",
      "metadata": {
        "id": "59db51f7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 11.2 Normality of residuals: Q‚ÄìQ plot + Shapiro‚ÄìWilk test\n",
        "plt.figure()\n",
        "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q‚ÄìQ Plot of Residuals\")\n",
        "plt.show()\n",
        "\n",
        "sh_stat, sh_p = stats.shapiro(residuals)\n",
        "print(f\"Shapiro‚ÄìWilk: statistic={sh_stat:.3f}, p-value={sh_p:.3g}\")\n",
        "print(\"Rule of thumb: p-value > 0.05 suggests residuals are close to normal (for inference).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff584073",
      "metadata": {
        "id": "ff584073"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 11.3 Independence of errors: Durbin‚ÄìWatson (‚âà2 is good; <1 or >3 indicates strong autocorrelation)\n",
        "dw = durbin_watson(residuals)\n",
        "print(f\"Durbin‚ÄìWatson statistic = {dw:.3f}\")\n",
        "print(\"Guideline: ~2 means uncorrelated; much below 2 ‚áí positive autocorrelation; much above 2 ‚áí negative autocorrelation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e46ecc",
      "metadata": {
        "id": "06e46ecc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 11.4 Homoscedasticity formal test: Breusch‚ÄìPagan\n",
        "# Use original predictors (X_test). Add constant for the test design matrix.\n",
        "\n",
        "X_bp = sm.add_constant(pd.DataFrame(X_test, columns=numeric_cols))\n",
        "bp_stat, bp_p, _, _ = het_breuschpagan(residuals, X_bp)\n",
        "print(f\"Breusch‚ÄìPagan: stat={bp_stat:.3f}, p-value={bp_p:.3g}\")\n",
        "print(\"Rule of thumb: p-value > 0.05 ‚áí no strong evidence of heteroscedasticity.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0ef9de2",
      "metadata": {
        "id": "f0ef9de2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 11.5 Multicollinearity: VIF (on training predictors)\n",
        "X_vif = pd.DataFrame(X_train, columns=numeric_cols).dropna()\n",
        "X_vif_const = sm.add_constant(X_vif, has_constant='add')\n",
        "\n",
        "vif_vals = []\n",
        "for i, col in enumerate(X_vif_const.columns):\n",
        "    if col == 'const':\n",
        "        continue\n",
        "    vif_vals.append({\"feature\": col, \"VIF\": variance_inflation_factor(X_vif_const.values, i+1)})\n",
        "\n",
        "vif_df = pd.DataFrame(vif_vals).sort_values(\"VIF\", ascending=False)\n",
        "vif_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "901800b4",
      "metadata": {
        "id": "901800b4"
      },
      "source": [
        "\n",
        "### üìù TODO\n",
        "- If **Breusch‚ÄìPagan p < 0.05**, try transforming a skewed feature (e.g., `np.log1p(x)`) and re-fit.  \n",
        "- If **Shapiro p < 0.05**, consider outliers or feature transforms.  \n",
        "- If **VIF > 10** for a feature, try removing it or combining highly correlated features.  \n",
        "- If **Durbin‚ÄìWatson** is far from 2 (time-indexed data), try adding lag features or using time-series models.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}